- Database
By preference, between MySQL and PostgreSQL I'd use the latter, for several
reasons:
    - Security updates: Oracle pushes non backportable security updates
      embedding them into new releases and specifying in the changelog that
      there were security fixes, without further specifications.
    - MySQL has different storage engines with different pros and cons, that
      must be taken into account.
    - Some storage engines don't support some constraints, and by default MySQL
      generates only a warning when constraints aren't met.
    - PostgreSQL privileges can be configured by root editing some text
      configuration files, and configurations to allow everything on unix
      socket for some users and to perform authentication on TCP can make
      maintenance easier.
    - PostgreSQL has sequences, which can be useful in other contexts than
      auto-increment.

- HTTP server
Well, depends on what are the other requirements, in general Apache2 offers lots
of modules and functionalities so if those are required it would offer a good
choice.
But Apache2 is also quite slow compared to other servers such as nginx, so
lacking other requirements I'd go for a fast webserver.

- Caching
Should make sure that whenever possible, all the views provide a Last-Modified
or an ETag header. If possible and it is known in advance, an Expires header
could be used as well to avoid contacting the application server at all.
Unfortunately this approach would not grant correctness with the present
application.
This allows the clients to perform their caching.
In case the load is still too high, a reverse-proxy should be used to further
reduce the load.

- Monitoring
For the traffic I would use awstats, possibly running it on different sets of
logs.
I normally keep the RSS feed from Debian (which is what i use) to be notified
about security problems, in order to be able to update as soon as possible.
I'd also have some scripts checking auth.log to see for suspicious activity.
Normally using ssh on a different port gets rid of all the noise generated by
dumb bots trying to guess easy usernames and passwords.
With some more time I would configure nagios on the network.
If needed, firewalls could be configured to block too many connections coming
from the same source.

- Scaling
Originally the database, application server and webserver could be moved to
different hosts, to get more performances, as said, a reverse proxy could be
used (I'd go for nginx here as well), and to offload even more, the DNS could
be configured to use different addresses in rotation.
This would mean that probably also the database would need to be duplicated.
In general solutions that are less strict on correctness are faster.
To manage the servers clusterssh could be used.
To daily deploy the code, well it depends on how much code to deploy there is.
On facebook they use bittorrent because they distribute a binary blob containing
all the code and the apache web server.